import os
import torch
import pandas as pd
from datasets import Dataset, concatenate_datasets
from sklearn.model_selection import train_test_split
import librosa
from transformers import AutoProcessor, Qwen2AudioForConditionalGeneration, BitsAndBytesConfig
from peft import PeftModel
from tqdm import tqdm
from jiwer import wer
import warnings
import re # ì •ê·œí‘œí˜„ì‹ ì‚¬ìš©ì„ ìœ„í•´ ì¶”ê°€

import os
import math, random, torch
from torch.utils.data import Dataset, DataLoader, RandomSampler, BatchSampler
from omegaconf import OmegaConf

from nemo.collections.speechlm2 import SALM

from peft import LoraConfig, TaskType, get_peft_model

from no_think import NEW_CHAT_TEMPLATE

import pandas as pd
from datasets import Dataset, concatenate_datasets
from sklearn.model_selection import train_test_split
from torch.utils.data import IterableDataset
import librosa
from transformers import AutoProcessor,  TrainingArguments, Trainer, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

import torchaudio

from nemo.collections.common.prompts import PromptFormatter
from nemo.collections.common.tokenizers import AutoTokenizer
from nemo.collections.speechlm2.data.salm_dataset import left_collate_vectors

from transformers import Trainer as HFTrainer
import torch.nn.functional as F

# ë¶ˆí•„ìš”í•œ ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
warnings.filterwarnings("ignore")

# --- 1. ëª¨ë¸ ë° í‰ê°€ ë°ì´í„° ê²½ë¡œ ì„¤ì • ---
# BASE_MODEL_ID = "Qwen/Qwen2-Audio-7B-Instruct"
BASE_MODEL_ID = "/data3/gkook/model/canary-qwen-2.5b"
# í•™ìŠµëœ LoRA ì–´ëŒ‘í„°ì˜ ìµœì¢… ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œë¥¼ ì§€ì •í•©ë‹ˆë‹¤.
# ì´ ê²½ë¡œëŠ” ì´ì „ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì˜ `output_dir` ë‚´ì— ìˆëŠ” ë§ˆì§€ë§‰ `checkpoint-XXXXX` í´ë”ì…ë‹ˆë‹¤.
# LORA_ADAPTER_PATH = '/data3/gkook/temp/agi/canary-qwen-2.5b_ft_result/final/lora_adapters/new_ft'
# LORA_ADAPTER_PATH = '/data3/gkook/temp/agi/canary-qwen-2.5b_ft_result_v2/final/lora_adapters/new_ft'
# LORA_ADAPTER_PATH = '/data3/gkook/temp/agi/canary-qwen-2.5b_ft_result_v3/final/lora_adapters/new_ft'
LORA_ADAPTER_PATH = '/data3/gkook/temp/agi/canary-qwen-2.5b_ft_result_v4/final/lora_adapters/new_ft'
# ë°ì´í„°ì…‹ ê²½ë¡œ
LIBRI_TRAIN_CLEAN_100_PATH = "/data1/jc/AGI/LibriSpeech/LibriSpeech/train/LibriSpeech/train-clean-100"
LIBRI_TRAIN_CLEAN_360_PATH = "/data1/jc/AGI/LibriSpeech/LibriSpeech/train/LibriSpeech/train-clean-360"
OURS_CSV_PATH = "/data1/gkook/agi/tts_generated.csv"

# **ìƒˆë¡œ ì¶”ê°€ëœ LibriSpeech ê³µì‹ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ê²½ë¡œ**
LIBRI_TEST_CLEAN_PATH = "/data1/jc/AGI/LibriSpeech/LibriSpeech/test-clean" # ì´ ê²½ë¡œë¥¼ í™•ì¸í•˜ê³  ì •í™•íˆ ë§ì¶°ì£¼ì„¸ìš”.
LIBRI_TEST_OTHER_PATH = "/data1/jc/AGI/LibriSpeech/LibriSpeech/test-other" # í•„ìš” ì‹œ ì¶”ê°€ (í˜„ì¬ëŠ” test-cleanë§Œ ì‚¬ìš© ì˜ˆì •)


# --- 2. ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë“œ ---
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤: {device}")

# ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ
print(f"Loading base model {BASE_MODEL_ID}")
model = SALM.from_pretrained(BASE_MODEL_ID).bfloat16()

print_trainable_param = 0
print_total_param = 0
for n , p in model.named_parameters():
    if p.requires_grad==True:
        print('trainable', n, p.numel())
        print_trainable_param += p.numel()
    else:
        print('not trainable', n, p.numel())
    print_total_param += p.numel()
print(f"Trainable parameters: {print_trainable_param}")
print(f"Total parameters: {print_total_param}")

model.llm = model.llm.merge_and_unload()

# LoRA ì–´ëŒ‘í„° ê²°í•©
print(f"Loading and merging LoRA adapter from {LORA_ADAPTER_PATH}...")
model.llm = PeftModel.from_pretrained(model.llm, LORA_ADAPTER_PATH)
model.llm = model.llm.merge_and_unload() # í‰ê°€ ì‹œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ë° ì„±ëŠ¥ì„ ìœ„í•´ ë³‘í•©


model.eval() # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
model.to(device)
# model.tokenizer.tokenizer.chat_template = NEW_CHAT_TEMPLATE
print("Fine-tuned model loaded successfully.")


# --- 3. ë°ì´í„°ì…‹ ë¡œë“œ í•¨ìˆ˜ (í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì™€ ë™ì¼í•˜ì§€ë§Œ, ê³µì‹ LibriSpeech ë¡œë“œ ë¡œì§ ì¶”ê°€) ---
def load_librispeech_dataset(base_path, is_official_test=False):
    data = []
    # LibriSpeechì˜ ë””ë ‰í† ë¦¬ êµ¬ì¡° (ì˜ˆ: base_path/speaker_id/chapter_id/...)ë¥¼ íƒìƒ‰í•©ë‹ˆë‹¤.
    for speaker_id in os.listdir(base_path):
        speaker_path = os.path.join(base_path, speaker_id)
        if not os.path.isdir(speaker_path):
            continue
        for chapter_id in os.listdir(speaker_path):
            chapter_path = os.path.join(speaker_path, chapter_id)
            if not os.path.isdir(chapter_path):
                continue
            
            # .trans.txt íŒŒì¼ ì°¾ê¸°
            transcript_file_path = os.path.join(chapter_path, f"{speaker_id}-{chapter_id}.trans.txt")
            if os.path.exists(transcript_file_path):
                with open(transcript_file_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        parts = line.strip().split(" ", 1)
                        if len(parts) == 2:
                            audio_id, transcript = parts
                            audio_path = os.path.join(chapter_path, f"{audio_id}.flac")
                            if os.path.exists(audio_path):
                                data.append({"audio_path": audio_path, "text": transcript})
                            else:
                                # print(f"Warning: Audio file not found at {audio_path}") # ë„ˆë¬´ ë§ìœ¼ë©´ ì£¼ì„ ì²˜ë¦¬
                                pass
            else:
                # print(f"Warning: Transcript file not found at {transcript_file_path}") # ë„ˆë¬´ ë§ìœ¼ë©´ ì£¼ì„ ì²˜ë¦¬
                pass
    return data

def load_ours_dataset(csv_path):
    df = pd.read_csv(csv_path)
    data = []
    for _, row in df.iterrows():
        if os.path.exists(row["audio_path"]):
            data.append({"audio_path": row["audio_path"], "text": row["first_line"]})
        else:
            print(f"Warning: Audio file not found for ours dataset at {row['audio_path']}. Skipping.")
    return data


# --- 4. í‰ê°€ ë°ì´í„°ì…‹ ì¤€ë¹„ ---
print("Preparing evaluation datasets...")

# LibriSpeech í•™ìŠµ/ê²€ì¦ ë¶„í• ì€ ë” ì´ìƒ í•„ìš” ì—†ìœ¼ë¯€ë¡œ ì£¼ì„ ì²˜ë¦¬í•˜ê±°ë‚˜ ì œê±°í•©ë‹ˆë‹¤.
# ëŒ€ì‹  ê³µì‹ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
# librispeech_data_100 = load_librispeech_dataset(LIBRI_TRAIN_CLEAN_100_PATH)
# librispeech_data_360 = load_librispeech_dataset(LIBRI_TRAIN_CLEAN_360_PATH)
# librispeech_full_df = pd.DataFrame(librispeech_data_100 + librispeech_data_360)
# _, librispeech_test_df = train_test_split(librispeech_full_df, test_size=0.05, random_state=42)
# librispeech_test_dataset = Dataset.from_pandas(librispeech_test_df)

print(f"Loading official LibriSpeech test-clean dataset from {LIBRI_TEST_CLEAN_PATH}...")
librispeech_official_test_clean_data = load_librispeech_dataset(LIBRI_TEST_CLEAN_PATH, is_official_test=True)
librispeech_official_test_clean_dataset = Dataset.from_pandas(pd.DataFrame(librispeech_official_test_clean_data))
print(f"LibriSpeech Test-Clean loaded: {len(librispeech_official_test_clean_dataset)} samples.")


print(f"Loading ours dataset from {OURS_CSV_PATH} and splitting...")
ours_full_data = load_ours_dataset(OURS_CSV_PATH)
ours_full_df = pd.DataFrame(ours_full_data)
# í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì™€ ë™ì¼í•œ random_stateì™€ test_sizeë¥¼ ì‚¬ìš©í•˜ì—¬ ì •í™•íˆ ë™ì¼í•œ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
_, ours_test_df = train_test_split(ours_full_df, test_size=0.1, random_state=42)
ours_test_dataset = Dataset.from_pandas(ours_test_df)
print(f"Ours Test Dataset (from split) loaded: {len(ours_test_dataset)} samples.")

print("\n--- All evaluation datasets prepared. ---")

generation_kwargs = dict(
    do_sample=False,
    # temperature=0.7,      # 0.7~1.0 ë²”ìœ„ì—ì„œ ì¡°ì ˆ
    # top_p=0.9,            # ë˜ëŠ” top_k=50~100
    repetition_penalty=1.1,# 1.1~1.2
    no_repeat_ngram_size=3,# 3~4
    max_new_tokens=256,    # ê³¼ë„í•˜ë©´ ë°˜ë³µ í™•ë¥ â†‘
)


def evaluate_wer(dataset, model, dataset_name="Unnamed Dataset"):
    too_long_count = 0
    predictions = []
    references = []
    
    
    print(f"\n--- Starting WER Evaluation for: {dataset_name} ({len(dataset)} samples) ---")

    for i, item in enumerate(tqdm(dataset, desc=f"Evaluating {dataset_name}")):
        try:
            audio_path = item['audio_path']
            reference_text = item['text']
            with torch.no_grad():
                generated_ids = model.generate(
                    prompts = [
                            [
                                {
                                    # "role": "user", "content": f"/nothink Transcribe the following directly without other words: {model.audio_locator_tag}",
                                    "role": "user", "content": f"Transcribe the following: {model.audio_locator_tag}",
                                    "audio": [audio_path]
                                }
                            ]
                        ],
                    **generation_kwargs,
                )
                response_text = model.tokenizer.ids_to_text(generated_ids[0].cpu())
                
            # with torch.no_grad():
            #     prompt = f"Transcribe the following without thinking: {model.audio_locator_tag}"
                
            
            
            
            
            
            
            # print(f"Response text: {response_text}")
            # print(f"Original text: {reference_text}")
           
            # --- ì˜ˆì¸¡ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë¡œì§ (ê°•í™”ë¨) ---
            
            # predicted_text_cleaned = response_text.replace('<think>', '').replace('</think>', '').strip()
                        # --- ì˜ˆì¸¡ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë¡œì§ (ê°•í™”ë¨) ---
            predicted_text_cleaned = response_text.replace('<think>', '').replace('</think>', '').strip()
                            
            # print(f"Cleaned text : {predicted_text_cleaned}")
            # print(f"Original text: {reference_text}")            
            if len(predicted_text_cleaned) >= 2*len(reference_text) or '\n' in predicted_text_cleaned:
                print('Too long or new line Detected:', predicted_text_cleaned)
                too_long_count += 1
            predictions.append(predicted_text_cleaned)
            references.append(reference_text)

        except Exception as e:
            print(f"Error processing {item.get('audio_path', 'N/A')}: {e}. Skipping this sample.")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë¬¸ìì—´ì„ ì˜ˆì¸¡ìœ¼ë¡œ ì¶”ê°€í•˜ì—¬ WER ê³„ì‚°ì— ì˜í–¥ì„ ì£¼ì§€ ì•Šë„ë¡ í•©ë‹ˆë‹¤.
            # ì´ëŠ” WERì„ ê³¼ëŒ€í‰ê°€í•  ìˆ˜ ìˆì§€ë§Œ, ì „ì²´ ìƒ˜í”Œ ìˆ˜ ìœ ì§€ë¥¼ ìœ„í•¨ì…ë‹ˆë‹¤.
            predictions.append("") 
            references.append(item['text'])

    # jiwer.wer í•¨ìˆ˜ëŠ” ë‚´ë¶€ì ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì •ê·œí™”í•©ë‹ˆë‹¤ (ì†Œë¬¸ì ë³€í™˜, ê³µë°± ì²˜ë¦¬, ì¼ë¶€ êµ¬ë‘ì  ì œê±° ë“±).
    # ë”°ë¼ì„œ ì—¬ê¸°ì„œ ì¶”ê°€ì ì¸ .upper()ë‚˜ replace(" ", "")ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë¶ˆí•„ìš”í•˜ë©°, jiwerì˜ í‘œì¤€ ì •ê·œí™”ì— ë§¡ê¸°ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
    # ë‹¨, jiwerê°€ ì²˜ë¦¬í•˜ì§€ ì•ŠëŠ” íŠ¹ì • ì •ê·œí™”(ì˜ˆ: ìˆ«ì -> ë‹¨ì–´)ê°€ í•„ìš”í•˜ë‹¤ë©´, predictionsì™€ referencesë¥¼ WER ê³„ì‚° ì „ì— ë³€í™˜í•´ì•¼ í•©ë‹ˆë‹¤.
    wer_score = wer(references, predictions)
    return wer_score, predictions, references, too_long_count

# --- 6. WER í‰ê°€ ì‹¤í–‰ ---

# 1. LibriSpeech Official Test-Clean í‰ê°€
wer_score_librispeech, _, _, too_long_count = evaluate_wer(librispeech_official_test_clean_dataset, model, dataset_name="LibriSpeech Test-Clean (Official)")
print(f"\nğŸ“ˆ LibriSpeech Official Test-Clean WER: {wer_score_librispeech:.4f}")

print("-" * 50)

# 2. Ours Test Dataset í‰ê°€
wer_score_ours, _, _, too_long_count2 = evaluate_wer(ours_test_dataset, model, dataset_name="Ours Test Dataset (Internal Split)")
print(f"\nğŸ“ˆ Ours Test Dataset WER: {wer_score_ours:.4f}")

print("\n--- Evaluation Complete ---")
print('Too long or new line count LibriSpeech: ', too_long_count)
print('Too long or new line count Ours: ', too_long_count2)